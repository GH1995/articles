---
title: 建炎以来系年要录：感知机
tags:
  - machine learning
categories:
  - 统计学习方法
date: 2019-06-21 23:05:23
---

## 模型

$$
f(x)=\operatorname{sign}(w \cdot x+b)
$$

## 感知机的几何解释

$wx+b$ 对应于特征空间中的一个分离超平面 $S$，其中 $w$ 是 $S$ 的法向量，$b$ 是 $S$ 的截距。$S$ 将特征空间划分为两个部分，位于两个部分的点分别被分为正负两类。

## 策略（损失函数）

假设训练数据集是线性可分的，感知机的损失函数是误分类点到超平面 $S$ 的总距离。

$$
L(w, b)=-\sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)
$$

其中 M 是误分类点的集合。感知机学习的策略就是选取使损失函数最小的模型参数。

## 算法

$$
\begin{aligned} \nabla_{w} L(w, b) &=-\sum_{x_{i} \in M} y_{i} x_{i} \\ \nabla_{b} L(w, b) &=-\sum_{x_{i} \in M} y_{i} \end{aligned}
$$

随机选择一个误分类点 $(x_i, y_i)$，对 $w, b$ 进行更新，直到误差为0：

$$
\begin{array}{l}{w \leftarrow w+\eta y_{i} x_{i}} \\ {b \leftarrow b+\eta y_{i}}\end{array}
$$

该算法的直观解释是：当一个点被误分类，就调整 $w$，$b$ 使分离超平面向该误分类点接近。感知机的解可以不同。

## 对偶形式

假设原始形式中的 $w_0$ 和 $b_0$ 均为$0$，设逐步修改 $w$ 和 $b$ 共 $n$ 次，令 $a=n \eta$，最后学习到的 $w,b$ 可以表示为

$$
\begin{aligned} w &=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i} \\ b &=\sum_{i=1}^{N} \alpha_{i} y_{i} \end{aligned}
$$

那么对偶算法就变为设初始 $a$ 和 $b$ 均为$0$，每次选取数据更新 $a$ 和 $b$ 直至没有误分类点为止。对偶形式的意义在于可以将训练集中实例间的内积计算出来，存在 [Gram 矩阵](https://www.wikiwand.com/zh-hans/%E6%A0%BC%E6%8B%89%E5%A7%86%E7%9F%A9%E9%98%B5)中，可以大大加快训练速度。


