---
title: 建炎以来系年要录：优化算法小结
tags:
  - machine learning
  - todo
categories:
  - 统计学习方法
date: 2019-06-19 17:53:33
---

## 梯度下降法

1. 批量梯度下降(BGD)
$$
\theta_{j}^{\prime}=\theta_{j}+\frac{1}{m} \sum_{i=1}^{m}\left(y^{i}-h_{\theta}\left(x^{i}\right)\right) x_{j}^{i}
$$
2. 随机梯度下降(SGD)
$$
\theta_{j}^{\prime}=\theta_{j}+\left(y^{i}-h_{\theta}\left(x^{i}\right)\right) x_{j}^{i}
$$
3. 小批量梯度下降(MBGD)

## 牛顿法

牛顿法是二次收敛，因此收敛速度快。从几何上看是每次用一个二次曲面来拟合当前所处位置的局部曲面，而梯度下降法是用一个平面来拟合。

1. 黑塞矩阵是由目标函数 $f(x)$ 在点 $X$ 处的二阶偏导数组成的 $n \times n$ 阶对称矩阵。
2. 牛顿法：将 $f(x)$ 在 $x(k)$ 附近进行二阶泰勒展开：
$$
f(x)=f\left(x^{(k)}\right)+g_{k}^{\mathrm{T}}\left(x-x^{(k)}\right)+\frac{1}{2}\left(x-x^{(k)}\right)^{\mathrm{T}} H\left(x^{(k)}\right)\left(x-x^{(k)}\right)
$$
3. $g_k$是$f(x)$ 的梯度向量在 $x(k)$ 的值，$H(x(k))$ 是 $f(x)$ 的黑塞矩阵在点 $x(k)$ 的值。

$$
\nabla f(x)=g_{k}+H_{k}\left(x-x^{(k)}\right)
$$

$$
g_{k}+H_{k}\left(x^{(k+1)}-x^{(k)}\right)=0
$$


$$
x^{(k+1)}=x^{(k)}-H_{k}^{-1} g_{k}
$$


## 拟牛顿法

$$
g_{k+1}-g_{k}=H_{k}\left(x^{(k+1)}-x^{(k)}\right)
$$

$$
y_{k}=g_{k+1}-g_{k}
$$


$$
\delta_{k}=x^{(k+1)}-x^{(k)}
$$

### DFP算法

$$
G_{k+1}=G_{k}+P_{k}+Q_{k}
$$

$$
P_{k} y_{k}=\delta_{k}
$$

$$
Q_{k} y_{k}=-G_{k} y_{k}
$$

$$
P_{k}=\frac{\delta_{k} \delta_{k}^{T}}{\delta_{k}^{T} y_{k}}
$$

$$
Q_{k}=-\frac{G_{k} y_{k} y_{k}^{\mathrm{T}} G_{k}}{y_{k}^{\mathrm{T}} G_{k} y_{k}}
$$


$$
G_{k+1}=G_{k}+\frac{\delta_{k} \delta_{k}^{\tau}}{\delta_{k}^{T} y_{k}}-\frac{G_{k} y_{k} y_{k}^{T} G_{k}}{y_{k}^{T} G_{k} y_{k}}
$$

### BFGS算法

$$
B_{k+1} \delta_{k}=y_{k}
$$

$$
B_{k+1}=B_{k}+P_{k}+Q_{k}
$$

$$
P_{k} \delta_{k}=y_{k}
$$

$$
Q_{k} \delta_{k}=-B_{k} \delta_{k}
$$

$$
B_{k+1}=B_{k}+\frac{y_{k} y_{k}^{\mathrm{T}}}{y_{k}^{\mathrm{T}} \delta_{k}}-\frac{B_{k} \delta_{k} \delta_{k}^{\mathrm{T}} B_{k}}{\delta_{k}^{\mathrm{T}} B_{k} \delta_{k}}
$$
