---
title: '狸花猫记(2): 马尔可夫决策过程'
tags:
  - reinforcement learning
categories:
  - reinforcement learning
date: 2019-07-18 00:08:51
---

## 强化学习引入 MDP 的原因

第一、限定环境的状态转换模型 $P^a_{s s'}$

> 假设转化到下一个状态 $s'$ 的概率仅与上一个的状态 $s$ 有关

$$
P_{ss'}^a = \mathbb{E}(S_{t+1}=s'|S_t=s, A_t=a)
$$

第二、限定策略 $\pi$

> 状态 $s$ 时采取动作 $a$ 的概率仅与当前状态 $s$ 有关

$$
\pi(a|s) = P(A_t=a | S_t=s)
$$

第三、限定价值函数 $v_{\pi}(s)$ 

> $v_{\pi}(s)$ 仅依赖当前状态

这里引入一个概念**Gain**

$G_t$ 是从一个 MDP 从一个状态 $S_t$ 开始采样直到终止状态时所有衰减奖励之和，这是**总回报**。

> $G_t$ 是一条轨迹的总回报。这条轨迹也被称为回合(Episode)。
> 我们的目标函数是最大化**期望回报**，即希望智能体的每个动作的平均回报都是最大的，这个平均针对的不是回合数，而是动作数。

**这里有点类似于从卷积中采样，不过是反向的。**

$$
v_{\pi}(s) = \mathbb{E}_{\pi}(G_t|S_t=s ) = \mathbb{E}_{\pi}(R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3}+...|S_t=s)
$$

## MDP 的价值函数与贝尔曼方程

缺点：没有考虑动作 $a$ 带来的价值影响，有可能做了不同的动作都导致了 $s \to s'$


- 状态价值函数 $v_{\pi}(s)$
- 动作价值函数 $q_{\pi}(s,a)$

注意，这里多引入了一个 $a$，本质上 $v$ 和 $q$ 都是 $R_{t}$ 的采样

$$
q_{\pi}(s,a) = \mathbb{E}_{\pi}(G_t|S_t=s, A_t=a) = \mathbb{E}_{\pi}(R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3}+...|S_t=s,A_t=a)
$$

注意了，这里我们要推导价值函数基于状态的递推关系，注意这两个地方，第一是**价值函数**，第二是**状态**

$$
\begin{aligned} v_{\pi}(s) &=\mathbb{E}_{\pi}\left(R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots | S_{t}=s\right) \\ &=\mathbb{E}_{\pi}\left(R_{t+1}+\gamma\left(R_{t+2}+\gamma R_{t+3}+\ldots\right) | S_{t}=s\right) \\ &=\mathbb{E}_{\pi}\left(R_{t+1}+\gamma G_{t+1} | S_{t}=s\right) \\ &=\mathbb{E}_{\pi}\left(R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) | S_{t}=s\right) \end{aligned}
$$

这里我们发现随着状态转换$S_t \to S_{t+1}$，价值函数也满足某种递推关系$v_{\pi}(S_t) \to v_{\pi}(S_{t+1})$，**这是符合直觉的**

$$
v_{\pi}(s) = \mathbb{E}_{\pi}(R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t=s)
$$

这个式子是不符合直觉的，叫做**贝尔曼方程**，它说一个状态的价值由 **该状态的奖励**$+$ **后续状态价值**$\times$**衰减比例** 构成。评价一个时代的好坏不仅要看当代人的评价还有看后代人的评价，这是符合历史直觉的。

> 为什么不参考前面的状态？我觉得这是一个缺点，比如评价历史，前代积贫积弱，当代横扫寰宇，反差越大，评价越高。由魏到晋易，由隋到唐难，天下民生凋敝，征战不休，秦王翦灭诸侯，开创贞观之治，这个 reward 应该非常大才对。而晋武帝承魏旧章，吞并吴蜀并不是什么难事，reward
应该小一点才对。**把后人的功劳算给前人是不符合直觉的，把李世民的功劳算给隋炀帝是不对的。**

同理可得动作价值函数 $q_{\pi}(s,a)$ 的贝尔曼方程

$$
q_{\pi}(s, a)=\mathbb{E}_{\pi}\left(R_{t+1}+\gamma q_{\pi}\left(S_{t+1}, A_{t+1}\right) | S_{t}=s, A_{t}=a\right)
$$


## 状态价值函数与动作价值函数的递推关系

### 用动作价值函数表示状态价值函数

状态价值函数是所有动作价值函数基于策略$\pi$ 的期望，这是基于两个定义得到的。

状态价值函数 = 所有状态动作价值 $\times$ 动作概率[ 策略 ]


$$
v_{\pi}(s)=\sum_{a \in A} \pi(a | s) q_{\pi}(s, a)
$$

策略$\pi$的本质也就是动作$a$

### 用状态价值函数表示动作价值函数

动作价值函数 = 即时奖励 + 衰减 $\times \sum$ 下一状态的概率$P^a_{ss'}$ $\times$ 状态价值$v_{\pi}$

$$
q_{\pi}(s, a)=R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P_{s s^{\prime}}^{a} v_{\pi}\left(s^{\prime}\right)
$$


## 最优价值函数

如何比较策略的优劣呢？一般是通过对应的价值函数来比较的，因为价值函数是历史的累积，历史的最优。

$$
v_{*}(s)=\max _{\pi} v_{\pi}(s)
$$

下面的式子不用记

$$
\begin{aligned} v_{*}(s) &=\max _{a}\left(R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P_{s s^{\prime}}^{a} v_{*}\left(s^{\prime}\right)\right) \\ q_{*}(s, a) &=R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P_{s s^{\prime}}^{a} \max _{a^{\prime}} q_{*}\left(s^{\prime}, a^{\prime}\right) \end{aligned}
$$

状态策略有限而且不多的问题都能用这两个方程搞出来。


## MDP 小结

todo
