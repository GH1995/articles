---
title: 狸花猫记(1)：模型基础
tags:
  - Reinforcement Learning
categories:
  - reinforcement learning
---

## 强化学习在机器学习中的位置

- 与监督学习相比，数据有时间关系，奖励值延迟
- 与非监督学习相比，数据有时间关系，输出奖励值

## 强化学习建模

![Zq5uRJ.png](https://s2.ax1x.com/2019/07/17/Zq5uRJ.png)

强化学习要素

### 三个基本要素

1. 环境状态 $S_t$
2. 动作 $A_t$
3. 环境的奖励 $R_t$，这是一次性的**延迟的**，在状态 $S_{t-1}$ 采取的动作 $A_{t-1}$ 对应奖励 $R_t$ 

### 五个附加要素

1. 个体的策略$\pi$，通常表示为一个**条件$s$概率分布**，状态 $s$ 时采取动作$a$的概率，即 $\pi(a|s) = P(A_t=a | S_t=s)$**动作只由状态决定，$\pi$就像游戏攻略一样**
2. 采取行动后的价值 $v_{\pi}(s)$ 在给定策略$\pi$和状态$s$后，采取一系列行动后奖励累加就是价值，一般是个期望函数。当前作用给出的延时奖励是$R_{t+1}$。**所以把价值函数表示为$\sum \gamma R_i$，这里不关注$s \to s'$ 是怎么变的，只是在改变给出的$R_i$上不断求和**
$$
v_{\pi}(s) = \mathbb{E}_{\pi}(R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3}+...|S_t=s)
$$
3. 奖励衰减因子 $\gamma$ **越小越是短视，激进，大胆；越大越是周到，保守，谨慎**
4. 环境的状态转换模型 $P_{ss'}^a$，即在状态$s$下采取动作$a$,转到下一个状态$s′$的概率**描述了 $s \to s'$ 的过程**
5. 探索率 $\epsilon$ 有$\epsilon$ 不使用当前状态最优动作

## 强化学习的简单示例

![ZqaIVx.png](https://s2.ax1x.com/2019/07/17/ZqaIVx.png)

[玩一玩这个游戏](https://www.google.com/search?q=Tic-Tac-Toe&rlz=1C1GCEU_zh-CNHK857HK857&oq=t&aqs=chrome.2.69i57j0j69i59l2j69i60l2.5188j0j1&sourceid=chrome&ie=UTF-8)

环境状态 $S$ 九宫格，每个格子三种状态（1. 没棋子；2. 有x；3. 有 o），所以模型状态一共有 $3^9$ 种

动作 $A$，9 个格子，相当于 9 个动作选项，有棋子的不能下

$R$ 赢棋奖励 1，其他时候奖励有但是少。先手要低一些？？**感觉先手更有优势**

$\pi$ 学习得到，见代码

$v_{\pi}(s)$ 

$\gamma$ 设置为 0

$P^a_{s s'}$  选择之后状体确定，无需讨论

$\epsilon$ explore 
