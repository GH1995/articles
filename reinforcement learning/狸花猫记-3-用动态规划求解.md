---
title: '狸花猫记(3): 用动态规划求解'
categories:
  - reinforcement learning
date: 2019-07-18 01:40:57
tags:
---

## 动态规划和强化学习问题的联系

动态规划的定义：

- 问题的最优解可以由若干小问题的最优解构成**问题是洋葱的**
- 可以找到子问题状态之间的递推关系**洋葱是可剥的**

参考状态值函数贝尔曼方程

$$
v_{\pi}(s)=\sum_{a \in A} \pi(a | s)\left(R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P_{s s^{\prime}}^{a} v_{\pi}\left(s^{\prime}\right)\right)
$$

> 这是一个递推的式子，然而并没有什么卵用，状态空间太大了。动态规划是一条死路，启发式搜索或能获胜。


## 策略评估求解预测问题

**什么是策略评估？**

求解给定策略的状态价值函数

基本思路是，从任意一个状态价值函数开始，依据给定的策略，迭代更新状态价值函数，直至其收敛，得到该策略下最终的状态价值函数。


## 策略迭代求解控制问题

**什么是控制问题？**

求解最优的价值函数和策略

**什么是策略迭代？**

根据我们之前基于任意一个给定策略评估得到的状态价值来及时调整我们的动作策略

> 动态规划用贪婪法，这是一个愚蠢的策略。

## 价值迭代求解控制问题

可见由于策略调整，我们现在价值每次更新倾向于贪婪法选择的最优策略对应的后续状态价值，这样收敛更快。

只是动态规划如此。


## 异步动态规划算法

**定义**

每一次迭代并不对所有状态的价值进行更新，而是依据一定的原则有选择性的更新部分状态的价值

- 原位动态规划，不保留上一轮的计算状态
- 优先级动态规划，状态有优先级
- 实时动态规划，直接用个体与环境交互的实际经历来更新状态价值

## 动态规划求解强化学习问题小结

算法思路比较简单，主要就是利用贝尔曼方程来迭代更新状态价值，用贪婪法之类的方法迭代更新最优策略。

当问题规模很大的时候，动态规划算法将会因贝尔曼维度灾难而无法使用。因此我们还需要寻找其他的针对复杂问题的强化学习问题求解方法。
