---
title: '狸花猫记(5): 时序差分法'
tags:
  - reinforcement learning
categories:
  - reinforcement learning
date: 2019-07-18 21:55:49
---

蒙特卡罗法需要所有的采样序列都是经历完整的状态序列。如果没有完整的状态序列，那么就无法使用蒙特卡罗法求解了。

## 时序差分TD简介

> 没有完整的状态序列，只有部分的状态序列，那么如何可以近似求出某个状态的收获呢？

参考贝尔曼方程

$$
v_{\pi}(s) = \mathbb{E}_{\pi}(R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t=s)
$$

用$R_{t+1} + \gamma v(S_{t+1})$ 来近似代替 $G_t$

**TD 目标值**——$R_{t+1} + \gamma v(S_{t+1})$

**TD 误差**——$R_{t+1} + \gamma V(S_{t+1}) -V(S_t)$

用TD目标值近似代替收获 $G_t$ 的过程叫做**引导**，这样只需要两个连续的状态与对应的奖励，就可以尝试求解强化学习问题了。

## 时序差分TD的预测问题求解

## $n$步时序差分

## $TD(\lambda)$

## 时序差分的控制问题求解

## 时序差分小结

主流的强化学习方法，是许多方法的基础
