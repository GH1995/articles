---
title: 困学纪闻注：概率图模型——泥盆纪会议
date: 2019-06-20 09:45:56
tags:
    - deep learning
    - todo
---



图模型的基本问题 图模型有三个基本问题：

1. 表示问题：对于一个概率模型，如何通过图结构来描述变量之间的依赖关系。
2. 推断问题：在已知部分变量时，计算其它变量的后验概率分布。 
3. 学习问题：图模型的学习包括图结构的学习和参数的学习。在本章我们只关注在给定图结构时的参数学习，即参数估计问题。

## 模型表示

**带阴影的节点表示可观测到的变量，不带阴影的节点表示隐变量，连边表示两变量间的条件依赖关系**

### 有向图模型（贝叶斯网络）

联合概率可以分解为局部条件概率的连乘形式

$$
p(\mathbf{x})=\prod_{k=1}^{K} p\left(x_{k} | \mathbf{x}_{\pi_{k}}\right)
$$

### 常见有向图模型

#### 朴素贝叶斯分类器


$$
P(H | E)=\frac{P(E | H)}{P(E)} P(H)
$$

![朴素贝叶斯模型的图模型表示](https://s2.ax1x.com/2019/06/20/VjmnDP.png)


![Logistic回归](https://s2.ax1x.com/2019/06/20/VjnIoT.png)

$$
P(H | E)=\frac{P(E | H)}{P(E)} P(H)
$$

$$
p(y | \mathbf{x}, \theta) \propto p\left(y | \theta_{c}\right) \prod_{i=1}^{d} p\left(x_{i} | y, \theta_{i, y}\right) = P(H) \prod P(E|H)
$$

![朴素贝叶斯模型的图模型表示](https://s2.ax1x.com/2019/06/20/VjmnDP.png)

![Logistic回归](https://s2.ax1x.com/2019/06/20/VjnIoT.png)

#### 隐马尔可夫模型

![隐马尔可夫模型](https://s2.ax1x.com/2019/06/20/VjuVmt.png)

$$
p(\mathbf{x}, \mathbf{y}, \theta)=\prod_{t=1}^{T} p\left(y_{t} | y_{t-1}, \theta_{s}\right) p\left(x_{t} | y_{t}, \theta_{t}\right)
$$

本质上就是

$$
P(H|H_{t-1}) P(E|H)
$$

### 无向图模型

$$
p(\mathbf{x})=\frac{1}{Z} \prod_{c \in \mathcal{C}} \phi_{c}\left(\mathbf{x}_{c}\right)
$$

### 常见无向图模型

#### 对数线性模型

$$
p(y | \mathbf{x}, \theta)=\frac{1}{Z(\mathbf{x}, \theta)} \exp \left(\theta^{\mathrm{T}} f(\mathbf{x}, y)\right)
$$

![最大熵模型](https://s2.ax1x.com/2019/06/20/Vjuy0x.png)


#### 条件随机场
和最大熵模型不同，条件随机场建模的条件概率$p(\mathbf{y}|\mathbf{x})$中，$\mathbf{y}$一般为随机向量，因此需要对$p(\mathbf{y}|\mathbf{x})$进行因子分解。

$$
p(\mathbf{y} | \mathbf{x}, \theta)=\frac{1}{Z(\mathbf{x}, \theta)} \exp \left(\sum_{c \in \mathcal{C}} \theta_{c}^{\mathrm{T}} f_{c}\left(\mathbf{x}, \mathbf{y}_{c}\right)\right)
$$

![线性链的条件随机场](https://s2.ax1x.com/2019/06/20/VjuTBt.png)

## 推断

## 近似推断

### 蒙特卡罗方法（采样法）
蒙特卡罗方法的基本思想可以归结为根据一个已知概率密度函数为$p(x)$的分布来计算函数$f(x)$的期望

$$
\mathbb{E}[f(x)]=\int_{x} f(x) p(x) d x
$$

当$p(x)$比较复杂时，很难用解析的方法来计算这个期望。但是可以从$p(x)$中抽取$N$个样本，然后用均值来近似计算上述期望：
$$
\hat{f}_{N}=\frac{1}{N}\left(f\left(x^{(1)}\right)+\cdots+f\left(x^{(N)}\right)\right)
$$

蒙特卡罗方法的难点是如何进行随机采样，即如何让计算机生成满足概率密度函数$p(x)$的样本。

一般是先根据一个比较容易采样的分布进行采样，然后通过一些策略来间接得到符合$p(x)$分布的样本。

### 拒绝采样

我们可以引入一个容易采样的分布$q(x)$， 一般称为**提议分布**，然后以某个标准来拒绝一部分的样本使得最终采集的样本服从分布$p(x)$。

已知未归一化的分布$\hat{p}(x)$，我们需要构建一个提议分布$q(x)$和一个常数$k$，使得$kq(x)$可以覆盖函数 $\hat{p}(x)$。

> 我们用这个好算的$q(x)$先操作一波，然后计算$\frac{\hat{p}(x)}{q(x)}$，这就是里面的有效成分！

对于每次抽取的样本$\hat{x}$，计算接受概率

$$
\alpha(\hat{x})=\frac{\hat{p}(\hat{x})}{k q(\hat{x})}
$$

### 重要性采样
略

### 马尔可夫链蒙特卡罗方法
略

#### MH算法
略

#### Metropolis 算法
略

#### 吉布斯采样

一种有效地对高维空间中的分布进行采样的MCMC方法

略

## 学习

### 不含隐变量的学习

极大似然估计

- 采样法
- 坐标上升法

### 含隐变量的参数估计

#### EM算法

一个样本$\mathbf{x}$的边际似然函数

$$
p(\mathbf{x} | \theta)=\sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z} | \theta)
$$

![带隐变量的贝叶斯网络:盘子表示法](https://s2.ax1x.com/2019/06/20/VjljI0.png)

训练集的对数边际似然为

$$
\begin{aligned} \mathcal{L}(\mathcal{D} | \theta) &=\frac{1}{N} \sum_{i=1}^{N} \log p\left(\mathbf{x}^{(i)}, \theta\right) \\ &=\frac{1}{N} \sum_{i=1}^{N} \log \sum_{\mathbf{z}} p\left(\mathbf{x}^{(i)}, \mathbf{z} | \theta\right) \end{aligned}
$$

为了计算$\log p(\mathbf{x}|\theta)$，我们引入$q(\mathbf{z})$为定义在隐变量$\mathbf{Z}$上的分布。

样本$\mathbf{x}$的对数边际似然函数为
$$
\begin{aligned} \log p(\mathbf{x} | \theta) &=\log \sum_{\mathbf{z}} q(\mathbf{z}) \frac{p(\mathbf{x}, \mathbf{z} | \theta)}{q(\mathbf{z})} \\ & \geq \sum_{\mathbf{z}} q(\mathbf{z}) \log \frac{p(\mathbf{x}, \mathbf{z} | \theta)}{q(\mathbf{z})} \\ & \triangleq E L B O(q, \mathbf{x} | \theta) \end{aligned}
$$
证据下界


由Jensen不等式的性质可知，仅当$q(\mathbf{z})=p(\mathbf{z} | \mathbf{x}, \theta)$ 时，取等式。

这样，最大化对数边际似然函数$\log p(\mathbf{x} | \theta)$ 的过程可以分解为两步：

1. 先找到近似分布$q(\mathbf{z})$ 使得$\log p(\mathbf{x} | \theta)=ELBO(q, \mathbf{x} | \theta)$
2. 再寻找参数 $\theta$最大化$ELBO(q, \mathbf{x} | \theta)$

#### 高斯混合模型

![高斯混合模型](https://s2.ax1x.com/2019/06/20/Vjdsu6.png)

略
