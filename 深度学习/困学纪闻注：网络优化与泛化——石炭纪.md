---
title: 困学纪闻注：网络优化与泛化——石炭纪
tags:
  - deep learning
  - todo
categories:
  - 深度学习
date: 2019-06-20 00:08:00
---

## 网络优化

1. 网络结构多样性
2. 高维变量的非凸优化
    - 平坦底部

## 优化算法

优化算法三种

1. mini-batch SGD
2. 学习率衰减
3. 梯度方向优化

优化算法面临的问题

1. 初始化参数
2. 预处理数据
3. 选择学习率


学习率衰减优化算法三种
1. AdaGrad: $g_t$参数的偏导数积累
2. RMSprop: $g_t$平方的指数衰减移动平均
3. AdaDelta: $\Delta \theta$平方的指数衰减移动平均

### AdaGrad
$$
G_{t}=\sum_{\tau=1}^{t} \mathbf{g}_{\tau} \odot \mathbf{g}_{\tau}
$$

$\mathbf{g}_{\tau} \in \mathbb{R}^{|\theta|}$ 是第$\tau$次迭代时的梯度

$$
\Delta \theta_{t}=-\frac{\alpha}{\sqrt{G_{t}+\epsilon}} \odot \mathrm{g}_{t}
$$

如果某个参数的偏导数累积比较大，其学习率相对较小

Adagrad算法的缺点是在经过一定次数的迭代依然没有找到最优点时，由 于这时的学习率已经非常小，很难再继续找到最优点。

### RMSprop

计算$\mathbf{g}_t$平方的指数衰减移动平均
$$
G_{t}=\beta G_{t-1}+(1-\beta) \mathbf{g}_{t} \odot \mathbf{g}_{t}
$$

$$
\Delta \theta_{t}=-\frac{\alpha}{\sqrt{G_{t}+\epsilon}} \odot \mathbf{g}_{t}
$$

## 梯度方向优化

### 动量法

计算负梯度的“加权移动平均”作为参数的更新方向

$$
\Delta \theta_{t}=\rho \Delta \theta_{t-1}-\alpha \mathbf{g}_{t}
$$

### Adam

Adam算法一方面计算梯度平方$\mathbf{g}_t^2$的指数加权平均（和RMSprop类似），另一方面计算梯度$\mathbf{g}_t$的指数加权平均（和动量法类似）

$$
\begin{array}{c}{M_{t}=\beta_{1} M_{t-1}+\left(1-\beta_{1}\right) \mathbf{g}_{t}} \\ {G_{t}=\beta_{2} G_{t-1}+\left(1-\beta_{2}\right) \mathbf{g}_{t} \odot \mathbf{g}_{t}}\end{array}
$$

更新

$$
\Delta \theta_{t}=-\frac{\alpha}{\sqrt{\hat{G}_{t}+\epsilon}} \hat{M}_{t}
$$

### 梯度截断

- 按值截断
- 按模截断

## 参数初始化

- Gaussian分布初始化
- 均匀分布初始化

## 数据预处理

缩放归一化

$$
\hat{x}^{(i)}=\frac{x^{(i)}-\min _{i}\left(x^{(i)}\right)}{\max _{i}\left(x^{(i)}\right)-\min _{i}\left(x^{(i)}\right)}
$$

标准归一化

$$
\hat{x}^{(i)}=\frac{x^{(i)}-\mu}{\sigma}
$$

白化(PAC)

## 逐层归一化

略，这节我看不懂

## 超参数优化

1. 网格搜索
2. 随机搜索
3. 贝叶斯优化
4. 动态资源分配
5. 神经网络架构

## 网络正则化

### $l_1$和$l_2$正则化

$$
\begin{array}{c}{\theta^{*}=\underset{\theta}{\arg \min } \frac{1}{N} \sum_{n=1}^{N} \mathcal{L}\left(y^{(n)}, f\left(\mathbf{x}^{(n)}, \theta\right)\right)} \\ {\text { subject to  } \ell_{p}(\theta) \leq 1}\end{array}
$$

$$
\ell_{1}(\theta)=\sum_{i} \sqrt{\theta_{i}^{2}+\epsilon}
$$

弹性网络正则化：同时加入$l_1$和$l_2$

### 权重衰减

### 提前停止

### 丢弃法

### 数据增强

### 标签平滑

即在输出标签中添加噪声来 避免模型过拟合

上面的标签平滑方法是给其它$K − 1$个标签相同的概率 $\frac{\epsilon}{K-1}$
