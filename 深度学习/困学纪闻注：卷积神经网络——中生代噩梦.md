---
title: 困学纪闻注：卷积神经网络——中生代噩梦
tags:
  - deep learning
  - todo
categories:
  - 深度学习
date: 2019-06-19 21:49:06
---

## 卷积

![卷积](https://s2.ax1x.com/2019/06/19/VXcsyj.png)

$$
y_{t}=\sum_{k=1}^{m} w_{k} \cdot x_{t-k+1}
$$

一幅图像在经过卷积操作后得到结果称为特征映射（Feature Map）。

互相关和卷积的区别在于卷积核仅仅是否进行翻转。

### 卷积层的神经元数量

![神经元数量](https://s2.ax1x.com/2019/06/19/VXcz1e.png)

## 卷积神经网络

### 卷积层

![第$l$层神经元数量](https://s2.ax1x.com/2019/06/19/VXgY3F.png)

$$
n^{(l)}=n^{(l-1)}-m+1
$$

![卷积层](https://s2.ax1x.com/2019/06/19/VXgWDA.png)

![卷积层的参数数量](https://s2.ax1x.com/2019/06/19/VX2C2F.png)

### 典型的卷积网络结构

![典型的卷积网络结构](https://s2.ax1x.com/2019/06/19/VX21rd.png)

## 几种典型的卷积神经网络

### Inception网络

一个卷积层包含多个不同大小的卷积操作，称为Inception模块。

Inception模块同时使用$1 × 1$、$3 × 3$、$5 × 5$等不同大小的卷积核，并将得到的特征映射在深度上拼接（堆叠）起来作为输出特征映射。

Inception模块在进行$3 × 3$、$5 × 5$的卷积之前、$3 × 3$的最大汇聚之后，进行一次$1×1$的卷积来减少特征映射的深度。如果输入特征映射 之间存在冗余信息，$1 × 1$的卷积相当于先进行一次特征抽取。

### ResNet

$$
h(\mathbf{x})=\mathbf{x}+(h(\mathbf{x})-\mathbf{x})
$$

让非线性单元$f(\mathbf{x}, \theta)$去近似残差函数$h(\mathbf{x})−\mathbf{x}$

## 其他卷积方式

**转置卷积**：用小图片和大卷积核生成特征映射，将低维特征映射到高维特征

**微步卷积**：给图片插入0

**空洞卷积**：给卷积核插入0

