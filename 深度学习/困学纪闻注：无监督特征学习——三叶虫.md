---
title: 无监督特征学习——三叶虫
tags:
  - deep learning
categories:
  - 深度学习
date: 2019-07-11 10:54:04
---

## 稀疏编码(Sparse Coding)

### 基本概念

**稀疏性**

外界信息经过编码后仅有一小部分神经元激活

**基向量**

$A=\left[\mathbf{a}_{1}, \cdots, \mathbf{a}_{p}\right]$

**输入样本**

$\mathbf{x} \in \mathbb{R}^{d}$

> 将输入样本表示为基向量的线性组合

$$
\begin{aligned} \mathbf{x} &=\sum_{i=1}^{p} z_{i} \mathbf{a}_{i} \\ &=A \mathbf{z} \end{aligned}
$$

**编码**

基向量的系数 $\mathbf{z}=\left[z_{1}, \cdots, z_{p}\right]$ 称为 输入样本 $\mathbf{X}$ 的编码，基向量也称为**字典**。

编码是对 $d$ 维空间中的样本 $\mathbf{x}$ 找到其在 $p$ 维空间中的表示（或投影），其目标通常是编码的各个维度都是统计独立的，并且可以重构出输入样本。


给定 $N$ 个输入向量 $\mathbf{x}^{(1)}, \cdots, \mathbf{x}^{(N)}$，其**稀疏编码的目标函数**定义为：
$$
L(A, Z)=\sum_{n=1}^{N}\left(\left\|\mathbf{x}^{(n)}-A \mathbf{z}^{(n)}\right\|^{2}+\eta \rho\left(\mathbf{z}^{(n)}\right)\right)
$$

- $A$ 字典
- $\mathbf{z}$ 编码
- $\rho(\cdot)$ 稀疏性衡量函数
- $\eta$ 控制稀疏性的强度

对于向量 $\mathbf{z}$，稀疏性定义为非零元素的比例，向量里越多的元素是 0 ，这个向量就越稀疏。$\mathbf{z}$ 越稀疏，$\rho(\mathbf{z})$ 打分越小，效果越好。

**稀疏性衡量函数**

$l_0$ 范式，不用

$$
\rho(\mathbf{z})=\sum_{i=1}^{p} \mathbf{I}\left(\left|z_{i}\right|>0\right)
$$

*可导性不强，难以优化*

**$l_1$ 范式**

$$
\rho(\mathbf{z})=\sum_{i=1}^{p}\left|z_{i}\right|
$$

**对数函数**

$$
\rho(\mathbf{z})=\sum_{i=1}^{p} \log \left(1+z_{i}^{2}\right)
$$

**指数函数**

$$
\rho(\mathbf{z})=\sum_{i=1}^{p}-\exp \left(-z_{i}^{2}\right)
$$

### 训练方法

给了 $N$ 个输入向量 $\mathbf{x}^{(1)}, \cdots, \mathbf{x}^{(N)}$

要学出

1. 基向量 $A$**字典**
2. 每个输入样本对应的稀疏编码 $\mathbf{z}^{(1)}, \cdots, \mathbf{z}^{(N)}$

训练过程：**交替优化**

1. 固定基向量 $A$ ，对于每个输入 $\mathbf{x}^{(n)}$，计算其最优编码 
$$
\min _{\mathbf{z}^{(n)}}\left\|\mathbf{x}^{(n)}-A \mathbf{z}^{(n)}\right\|^{2}+\eta \rho\left(\mathbf{z}^{(n)}\right)
$$
2. 固定 $n$ 个编码，计算最优基向量**字典**
$$
\min _{A} \sum_{n=1}^{N}\left(\left\|\mathbf{x}^{(n)}-A \mathbf{z}^{(n)}\right\|^{2}\right)+\lambda \frac{1}{2}\|A\|^{2}
$$

### 优点

- 计算量小
- 可解释性强
- 自动特征选择


- TODO: GH1995 -
