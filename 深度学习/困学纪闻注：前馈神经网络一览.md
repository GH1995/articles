---
title: 困学纪闻注：前馈神经网络一览——激活函数寒武纪
tags:
  - deep learning
  - todo
categories:
  - 深度学习
date: 2019-06-19 19:25:46
---

## 常见激活函数

### Sigmoid 型激活函数

1. Logistic 函数，值域$0 \sim 1$
$$
\sigma(x)=\frac{1}{1+\exp (-x)}
$$
2. Tanh 函数，值域$-1 \sim 1$
$$
\tanh (x)=2 \sigma(2 x)-1
$$


### 修正线性单元

1. ReLU
$$
\begin{aligned} \operatorname{ReLU}(x) &=\left\{\begin{array}{ll}{x} & {x \geq 0} \\ {0} & {x<0}\end{array}\right.\\ &=\max (0, x) \end{aligned}
$$
**优点**
**缺点** 非零中心化/死亡ReLU问题
2. LeakyReLU
在输入 $x < 0$时，保持一个很小的梯度$\lambda$
$$
\text{LeakyReLU}(x)=\left\{\begin{array}{ll}{x} & {\text { if  } x>0} \\ {\gamma x} & {\text { if  } x \leq 0}\end{array}\right.
$$
3. PReLU
引入一个可学习的参数，不同神经元可以有不同的参数
$$
\operatorname{PReLU}_{i}(x)=\left\{\begin{array}{ll}{x} & {\text { if  } x>0} \\ {\gamma_{i} x} & {\text { if  } x \leq 0}\end{array}\right.
$$
